{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation of the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an extended dataset:\n",
    "\n",
    "1. Get new data from https://www.bluebikes.com/system-data\n",
    "2. Download the trip zip files for each month of 2019\n",
    "3. Append them in one dataframe\n",
    "4. Download current station data from website\n",
    "\n",
    "All datasets used are provided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Import the required libraries for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# For Google Maps script\n",
    "import json\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Maps script\n",
    "API_KEY = ''\n",
    "GOOGLE_MAPS_DATA_FILE = \"./datasets/google_api_data.json\"\n",
    "\n",
    "# Data provided\n",
    "TRIP_DATA_PATH = './datasets/boston_2019.csv'\n",
    "WEATHER_DATA_PATH = './datasets/weather_hourly_boston.csv'\n",
    "\n",
    "# Station data from Blue Bikes website\n",
    "STATION_DATA_PATH = './datasets/current_bluebikes_stations.csv'\n",
    "\n",
    "# Load all csv for the different months in 2019 from Boston Bikes System Data\n",
    "TRIP_DATA_WEBSITE_JANUARY = './datasets/csv/201901-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_FEBRUARY = './datasets/csv/201902-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_MARCH = './datasets/csv/201903-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_APRIL = './datasets/csv/201904-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_MAY = './datasets/csv/201905-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_JUNE = './datasets/csv/201906-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_JULY = './datasets/csv/201907-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_AUGUST = './datasets/csv/201908-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_SEPTEMBER = './datasets/csv/201909-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_OCTOBER = './datasets/csv/201910-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_NOVEMBER = './datasets/csv//201911-bluebikes-tripdata.csv'\n",
    "TRIP_DATA_WEBSITE_DECEMBER = './datasets/csv/201912-bluebikes-tripdata.csv'\n",
    "\n",
    "\n",
    "# Assign datatypes for each dataframe\n",
    "TRIP_DTYPES = {\n",
    "    'start_time': 'datetime64',\n",
    "    'end_time': 'datetime64',\n",
    "    'start_station_id': 'int64',\n",
    "    'end_station_id': 'int',\n",
    "    'start_station_name': 'str',\n",
    "    'end_station_name': 'str',\n",
    "    'bike_id': 'int64',\n",
    "    'user_type': 'str',\n",
    "}\n",
    "\n",
    "TRIP_WEBSITE_DTYPES = {\n",
    "    'trip_duration': 'int64',\n",
    "    'start_time': 'datetime64',\n",
    "    'stop_time': 'datetime64',\n",
    "    'start_station_id': 'int64',\n",
    "    'start_station_name': 'str',\n",
    "    'start_station_lat': 'float64',\n",
    "    'start_station_lng': 'float64',\n",
    "    'end_station_id': 'int',\n",
    "    'end_station_name': 'str',\n",
    "    'end_station_lat': 'float64',\n",
    "    'end_station_lng': 'float64',\n",
    "    'bike_id': 'int64',\n",
    "    'user_type': 'str',\n",
    "    'birth_year': 'int64',\n",
    "    'gender': 'int64',\n",
    "    'user_type_id': 'int64'\n",
    "}\n",
    "\n",
    "WEATHER_DTYPES = {\n",
    "    'max_temp': 'float64',\n",
    "    'min_temp': 'float64',\n",
    "    'precip': 'int64'\n",
    "}\n",
    "\n",
    "STATION_DTYPES = {\n",
    "    'station_id_string': 'str',\n",
    "    'station_name': 'str',\n",
    "    'station_lat': 'float64',\n",
    "    'station_lng': 'float64',\n",
    "    'district': 'str',\n",
    "    'total_docks': 'int64',\n",
    "    'deployment_year': 'int64',\n",
    "    'station_id': 'int64',\n",
    "    'public': 'str',\n",
    "    'station_id': 'int64'\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Here we load the data into our notebook. \n",
    "\n",
    "We haves 3 types of data:\n",
    "\n",
    "1. Trip_data: data on individual trips, each represented by one record, such as time or station info\n",
    "2. Weather_data: data on temperature and rain for each hour\n",
    "3. Station_data: data for each station (e.g. number of docks or deployment year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all  provided csv data for trips, weather and stations into dataframes\n",
    "trip_data = pd.read_csv(TRIP_DATA_PATH)\n",
    "weather_data = pd.read_csv(WEATHER_DATA_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use station data from the Boston Bikes website, as it contains more information about each station that may be useful later, such as the number of docks or the year of deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the stations csv data from the Blue Bikes website\n",
    "station_data = pd.read_csv(STATION_DATA_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the trip data from the Boston Bikes website because it contains slightly more information about each trip, such as gender or year of birth.\n",
    "\n",
    "We load the trip data for each month and then concat them into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all csv trip data for each month into dataframes\n",
    "trip_data_website_january = pd.read_csv(TRIP_DATA_WEBSITE_JANUARY)\n",
    "trip_data_website_february = pd.read_csv(TRIP_DATA_WEBSITE_FEBRUARY)\n",
    "trip_data_website_march = pd.read_csv(TRIP_DATA_WEBSITE_MARCH)\n",
    "trip_data_website_april = pd.read_csv(TRIP_DATA_WEBSITE_APRIL)\n",
    "trip_data_website_may = pd.read_csv(TRIP_DATA_WEBSITE_MAY)\n",
    "trip_data_website_june = pd.read_csv(TRIP_DATA_WEBSITE_JUNE)\n",
    "trip_data_website_july = pd.read_csv(TRIP_DATA_WEBSITE_JULY)\n",
    "trip_data_website_august = pd.read_csv(TRIP_DATA_WEBSITE_AUGUST)\n",
    "trip_data_website_september = pd.read_csv(TRIP_DATA_WEBSITE_SEPTEMBER)\n",
    "trip_data_website_october = pd.read_csv(TRIP_DATA_WEBSITE_OCTOBER)\n",
    "trip_data_website_november = pd.read_csv(TRIP_DATA_WEBSITE_NOVEMBER)\n",
    "trip_data_website_december = pd.read_csv(TRIP_DATA_WEBSITE_DECEMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat datasets for each month from website into one dataframe\n",
    "trip_data_website = pd.concat([trip_data_website_january, trip_data_website_february, trip_data_website_march, trip_data_website_april, trip_data_website_may, trip_data_website_june, trip_data_website_july, trip_data_website_august, trip_data_website_september, trip_data_website_october, trip_data_website_november, trip_data_website_december], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the data\n",
    "### Trip data\n",
    "First, we rename the columns of the trip data from the website to appropriate names. Since there are only two types of users (subscribers and customers), we binarize them into their own column. This will make clustering easier later. Finally, we assign the dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename headers of website trip data\n",
    "trip_data_website = trip_data_website.rename(columns={'tripduration':'trip_duration', 'starttime':'start_time', 'stoptime':'stop_time', 'start station id':'start_station_id', 'start station name':'start_station_name', 'start station latitude':'start_station_lat', 'start station longitude':'start_station_lng', 'end station id':'end_station_id', 'end station name':'end_station_name', 'end station latitude':'end_station_lat', 'end station longitude':'end_station_lng', 'bikeid':'bike_id', 'usertype':'user_type', 'birth year':'birth_year', 'gender':'gender'})\n",
    "\n",
    "# Binarize the user type into own column (Subscriber = 1, Customer = 0) for both trip_data and trip_data_website\n",
    "print(trip_data_website['user_type'].unique())\n",
    "trip_data_website['user_type_id'] = trip_data_website.apply(lambda row: 1 if row.user_type == 'Subscriber' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dtype to trip data website\n",
    "trip_data_website = trip_data_website.astype(TRIP_WEBSITE_DTYPES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data\n",
    "The weather data provided ranges from 2015 to 2022. Here we only need the data for 2019, so we filter the data by year and sort it by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date_time to pandas datetime\n",
    "weather_data['date_time'] = pd.to_datetime(weather_data['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter weather data for 2019\n",
    "weather_data = weather_data[weather_data['date_time']<pd.to_datetime('2020-01-01 00:00:00')]\n",
    "weather_data = weather_data[weather_data['date_time']>=pd.to_datetime('2019-01-01 00:00:00')]\n",
    "\n",
    "# Sort weather data by date\n",
    "weather_data = weather_data.sort_values(by='date_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the weather data, we had to make some assumptions on how to interpret the data.\n",
    "\n",
    "**Assumption 1**: For the duplicates, we assume that the weather changes throughout the hour. However, since we have no information on which part of the hour the weather changes, we take the first record of each duplicate.\n",
    "\n",
    "**Assumption 2**: The time at which the ride begins is critical to the weather, since this is when the client decides to use the bike.\n",
    "\n",
    "**Assumption 3**: Since some hours are missing in weather_data, we assume that the weather has not changed during this time. Therefore, we will fill these hours with the previous weather conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and keep first record\n",
    "weather_data = weather_data.drop_duplicates(subset='date_time', keep='first')\n",
    "\n",
    "# Generate all hours for 2019 and merge with weather data\n",
    "hours_2019 = pd.DataFrame(pd.date_range(pd.to_datetime('2019-01-01 00:00:00'), pd.to_datetime('2019-12-31 23:00:00'), freq='H'), columns=['date_time'])\n",
    "weather_all_hours = pd.merge(how='outer', left=weather_data, right=hours_2019, on='date_time').sort_values(by='date_time').reset_index(drop=True)\n",
    "\n",
    "# Set date_time as index for dataframe\n",
    "weather_all_hours = weather_all_hours.set_index('date_time')\n",
    "\n",
    "# Fill missing values with forward fill and dtypes with backward fill\n",
    "weather_all_hours = weather_all_hours.fillna(method='ffill')\n",
    "weather_all_hours = weather_all_hours.fillna(method='bfill').astype(WEATHER_DTYPES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will merge the weather data into the trip data, so each trip has the weather associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_website = pd.merge_asof(trip_data_website.sort_values('start_time'), weather_all_hours.sort_index(), left_on='start_time', right_index=True, direction='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_website.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station data\n",
    "We can see that the header of station_data is not yet correct, the right values are in the first row. Therefore,  we need to set the first row as the header, delete the first record and reset the index. Then we will rename the columns to better fitting names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set header to first row values\n",
    "station_data.columns = station_data.iloc[0]\n",
    "\n",
    "# Remove first row by slicing\n",
    "station_data = station_data[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "station_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename headers\n",
    "station_data = station_data.rename(columns={'Number': 'station_id_string', 'Name': 'station_name', 'District': 'district', 'Total docks': 'total_docks', 'Deployment Year': 'deployment_year', 'Public': 'public', 'Latitude':'station_lat', 'Longitude':'station_lng'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the station data for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN in station_data\n",
    "station_data[station_data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN in each column\n",
    "print('There are ' + str(station_data.isnull().sum().sum()) + ' missing values in total.')\n",
    "print('There are ' + str(station_data['station_id_string'].isnull().sum()) + ' missing values in station id string.')\n",
    "print('There are ' + str(station_data['station_name'].isnull().sum()) + ' missing values in station name.')\n",
    "print('There are ' + str(station_data['station_lat'].isnull().sum()) + ' missing values in station lat.')\n",
    "print('There are ' + str(station_data['station_lng'].isnull().sum()) + ' missing values in station lng.')\n",
    "print('There are ' + str(station_data['district'].isnull().sum()) + ' missing values in district.')\n",
    "print('There are ' + str(station_data['public'].isnull().sum()) + ' missing values in public.')\n",
    "print('There are ' + str(station_data['total_docks'].isnull().sum()) + ' missing values in total docks.')\n",
    "print('There are ' + str(station_data['deployment_year'].isnull().sum()) + ' missing values in deployment year.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 9 missing values in total. \n",
    "\n",
    "Four of them in the district column. Therefore, we check where they appear and fill them with the correct value after a manual Google Maps search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the correct district\n",
    "station_data.loc[7].district='Somerville'\n",
    "station_data.loc[236].district='Cambridge'\n",
    "station_data.loc[262].district='Malden'\n",
    "station_data.loc[302].district='Salem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check district NaN\n",
    "print('There are ' + str(station_data['district'].isnull().sum()) + ' missing values in district.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deployment year, we can see that 3 of the 5 stations have a \"Temp Winter Station\" in their name. We assume that they are only used in winter each year and therefore have the current year as their year of deployment - in this case 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing deployment years\n",
    "station_data.loc[7].deployment_year=2022\n",
    "station_data.loc[83].deployment_year=2022\n",
    "station_data.loc[401].deployment_year=2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deployment NaN\n",
    "print('There are ' + str(station_data['deployment_year'].isnull().sum()) + ' missing values in deployment year.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the missing 2 values, we have no indication of when the year of deployment was. After searching the trip data for 'Boylston St at Charles St' and 'John Ahern Field at Kennedy-Longfellow School', we can see that there are no trips associated with either station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both stations there are no results for the starting and ending station\n",
    "print(trip_data_website[trip_data_website['start_station_name'].str.contains('Boylston St at Charles St')])\n",
    "print(trip_data_website[trip_data_website['end_station_name'].str.contains('Boylston St at Charles St')])\n",
    "print(trip_data_website[trip_data_website['start_station_name'].str.contains('John Ahern Field at Kennedy-Longfellow School')])\n",
    "print(trip_data_website[trip_data_website['end_station_name'].str.contains('John Ahern Field at Kennedy-Longfellow School')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we delete both stations from station_data and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Boylston St at Charles St (index=65) from station_data\n",
    "station_data = station_data.drop([65])\n",
    "\n",
    "# Drop John Ahern Field at Kennedy-Longfellow School (index=236) from station_data\n",
    "station_data = station_data.drop([236])\n",
    "\n",
    "# Reset index\n",
    "station_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, there are now no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "station_data[station_data.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are more stations in station_data than stations are appearing in trip_data. Therefore, we extract the stations from trip_data and merge them with the information available in the station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check amount of stations in station_data and trip_data\n",
    "print('There are ' + str(len(station_data['station_name'].unique())) + ' stations in the station data.')\n",
    "print('There are ' + str(len(trip_data_website['start_station_id'].unique())) + ' start stations in trip data.')\n",
    "print('There are ' + str(len(trip_data_website['end_station_id'].unique())) + ' end stations in trip data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique start and end stations\n",
    "start_stations = trip_data_website.drop_duplicates(subset=\"start_station_id\", keep=\"first\")[[\"start_station_id\",\"start_station_name\", \"start_station_lat\", \"start_station_lng\"]].sort_values(by=\"start_station_id\").reset_index(drop=True)\n",
    "end_stations = trip_data_website.drop_duplicates(subset=\"end_station_id\", keep=\"first\")[[\"end_station_id\",\"end_station_name\", \"end_station_lat\", \"end_station_lng\"]].sort_values(by=\"end_station_id\").reset_index(drop=True)\n",
    "\n",
    "# Merge all unique start and end stations into one dataframe\n",
    "all_stations = start_stations.merge(end_stations, left_on='start_station_id', right_on=\"end_station_id\", how='outer', suffixes=['', '_'], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN in all_stations\n",
    "all_stations[all_stations.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 5 stations with missing values - 2 start stations and 3 end stations. \n",
    "\n",
    "This means that the first two stations **only** appear as start stations and **never** appear as end stations, and the last 3 stations **only** appear as end stations and **never** appear as start stations, which explains the results above.\n",
    "\n",
    "The samples where no values for latitude and longitude are given are problematic, here ***MTL-ECO5.1-01*** (start), ***8D QC Station 02*** (end) and ***MTL-ECO4-01*** (end).\n",
    "\n",
    "We will now check in which of the trips the stations occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check all trips for starting station: MTL-ECO5.1-01\n",
    "# for index, row in trip_data_website.iterrows():\n",
    "#     if 'MTL-ECO5.1-01' in row['start_station_name']:\n",
    "#         print(index, row['start_station_name'], row['end_station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check all trips for ending station: 8D QC Station 02\n",
    "# for index, row in trip_data_website.iterrows():\n",
    "#     if '8D QC Station 02' in row['end_station_name']:\n",
    "#         print(index, row['start_station_name'], row['end_station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check all trips for ending station: MTL-ECO4-01\n",
    "# for index, row in trip_data_website.iterrows():\n",
    "#     if 'MTL-ECO4-01' in row['end_station_name']:\n",
    "#         print(index, row['start_station_name'], row['end_station_name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ***MTL-ECO5.1-01*** (start) and ***MTL-ECO4-01*** (end) there is one trip with the ID ***994366***. \n",
    "For ***8D QC Station 02*** (end) there is a trip from ***8D QC Station 01*** (start) with the ID ***50153***.\n",
    "\n",
    "We can also check with the stations dataset which of the stations above are appearing. We can neither find ***8D QC Station 02*** nor ***MTL-ECO4-01***. We can find ***MTL-ECO5.1-01*** and ***8D QC Station 01*** as listed stations.\n",
    "\n",
    "For ***8D QC Station 01*** we have latitude and longitude values, but if there is only one occurrence in trip_data, it is not so important, so we check it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check all trips for starting station: 8D QC Station 01\n",
    "# for index, row in trip_data_website.iterrows():\n",
    "#     if '8D QC Station 01' in row['start_station_name']:\n",
    "#         print(index, row['start_station_name'], row['end_station_name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one trip associated with each of the stations (***8D QC Station 01***, ***8D QC Station 02***, ***MTL-ECO5.1-01***, and ***MTL-ECO4-01***), they do not seem important to the scope of the task. Therefore, the trips are deleted from the trip data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop trips by id \n",
    "trip_data_website = trip_data_website.drop([50153])\n",
    "trip_data_website = trip_data_website.drop([994366])\n",
    "\n",
    "# Reset index\n",
    "trip_data_website.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove the stations listed above from the all_stations dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop stations by id (8D QC Station 01 id=202, MTL-ECO5.1-01 id=278, 8D QC Station 02 id=339, MTL-ECO4-01 id=340)\n",
    "all_stations = all_stations.drop([202])\n",
    "all_stations = all_stations.drop([278])\n",
    "all_stations = all_stations.drop([339])\n",
    "all_stations = all_stations.drop([340])\n",
    "\n",
    "# Reset index\n",
    "all_stations.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "all_stations[all_stations.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is one more end station than start station, we will use the end stations as the basis for our station data. \n",
    "\n",
    "We can omit the columns *start_station_id*, *start_station_name*, *start_station_lat* and *start_station_lng* because they provide the same information as the end station values. Then we rename the columns to appropriate names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We will drop the columns start_station_id, start_station_name, start_station_lat, start_station_lng as they are not needed\n",
    "all_stations = all_stations.drop(['start_station_id', 'start_station_name', 'start_station_lat', 'start_station_lng', '_merge' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "all_stations = all_stations.rename(columns={'end_station_id': 'station_id', 'end_station_name': 'station_name', 'end_station_lat': 'station_lat', 'end_station_lng': 'station_lng'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, all the stations that show up in the trip data are merged into a new dataframe with the additional information we have about each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe from station_data and all_stations\n",
    "total_station_data = station_data.drop(['station_lat', 'station_lng'], axis=1).merge(all_stations, right_on='station_name', left_on='station_name', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check again for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "total_station_data[total_station_data.isnull().any(axis=1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few samples in the trip data from 2019 that are not included in the current list of stations. \n",
    "\n",
    "As we do not have more information about the stations even after checking the other station files from Boston Bikes website, we decide to fill the NaN data with proper values.\n",
    "\n",
    "1) *total_docks* and *deployment_year* will be filled with -1, implying there is no info.\n",
    "2) *station_id_string* will be filled with the predefined format (X00000).\n",
    "3) For *public*, we noticed that the only values used are 'Yes' and 'nan'. Therefore, we assume that all known stations are public.\n",
    "4) For *district*, we will perform a Google Maps search to fill the district info (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill station data station id string with format X000000\n",
    "total_station_data['station_id_string'] = total_station_data['station_id_string'].fillna('X00000')\n",
    "\n",
    "# Fill station data public with 'Yes'\n",
    "total_station_data['public'] = total_station_data['public'].fillna('Yes')\n",
    "\n",
    "# Fill station data deployment year and total docks with -1\n",
    "total_station_data['deployment_year'] = total_station_data['deployment_year'].fillna(-1)\n",
    "total_station_data['total_docks'] = total_station_data['total_docks'].fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in total_station_data\n",
    "total_station_data[total_station_data.isnull().any(axis=1)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dtype to total_station_data station_id\n",
    "total_station_data.station_id = total_station_data.station_id.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Script to lookup the station names in Google Maps through their Find Place API\n",
    "# rows_with_nan = total_station_data[total_station_data.isnull().any(axis=1)]\n",
    "\n",
    "# def get_google_place_data(stations):\n",
    "#     station_info = {}\n",
    "#     url_endpoint = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "\n",
    "#     for index, record in stations.iterrows():\n",
    "#         url_query_params = f\"?input={urllib.parse.quote_plus(record.station_name)}&inputtype=textquery&fields=formatted_address%2Cname%2Cgeometry&key={API_KEY}&point:42.387151,-71.075978\"\n",
    "#         url = url_endpoint + url_query_params\n",
    "#         payload={}\n",
    "#         headers = {}\n",
    "#         response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "#         response = json.loads(response.text)\n",
    "#         if not response[\"candidates\"]:\n",
    "#             print(f\"--------{index}___{record.station_name}\")\n",
    "#             url_query_params = f\"?input={urllib.parse.quote_plus(record.station_name)}&inputtype=textquery&fields=formatted_address%2Cname%2Cgeometry&key={API_KEY}\"\n",
    "#             url = url_endpoint + url_query_params\n",
    "#             print(url)\n",
    "#             response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "#             response = json.loads(response.text)\n",
    "#         station_info[record.station_id] = response\n",
    "#     return station_info\n",
    "\n",
    "# google_api_data = get_google_place_data(rows_with_nan)\n",
    "\n",
    "# # Saving the data in a JSON-file\n",
    "\n",
    "# with open(GOOGLE_MAPS_DATA_FILE, 'w') as fp:\n",
    "#     json.dump(google_api_data, fp)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the json file created via the Google Maps API and the station data (see below), we find that in most cases it works as expected. For some stations, the values are not correct, e.g. locations were found in the UK or other parts of the US. Therefore, we will override these values with a manual Google Maps search with latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will fill in some districts manually through their lat and long values as they were not found correct via the Google Maps Find Place API\n",
    "total_station_data.at[18, 'district']='Boston' # Prudential Center - Belvedere St\n",
    "total_station_data.at[44, 'district']='Boston' # Congress St at North St\n",
    "total_station_data.at[57, 'district']='Boston' # Allston Green District - Griggs St at Commonwealth Ave\n",
    "total_station_data.at[70, 'district']='Somerville' # Beacon St at Washington / Kirkland\n",
    "total_station_data.at[76, 'district']='Brookline' # Brookline Village - Pearl Street at MBTA\n",
    "total_station_data.at[78, 'district']='Cambridge' # Inman Square at Vellucci Plaza / Hampshire St\n",
    "total_station_data.at[82, 'district']='Boston' # University of Massachusetts Boston - Integrated Sciences Complex 100\n",
    "total_station_data.at[91, 'district']='Somerville' # Ball Sq\n",
    "total_station_data.at[100, 'district']='Cambridge' # Harvard University Gund Hall at Quincy St / Ki...\n",
    "total_station_data.at[160, 'district']='Cambridge' # Sidney Research Campus/ Erie Street at Waverly\n",
    "total_station_data.at[171, 'district']='East Cambridge' # Child St at North St\n",
    "total_station_data.at[174, 'district']='Boston' # Upham's Corner T Stop - Magnolia St at Dudley St\n",
    "total_station_data.at[209, 'district']='Boston' # Washington St at Bradlee St\n",
    "total_station_data.at[224, 'district']='Newton' # Washington St at Myrtle St\n",
    "total_station_data.at[225, 'district']='Somerville' # 30 Dane St.\n",
    "total_station_data.at[261, 'district']='Boston' #  Broadway T Stop W\n",
    "total_station_data.at[280, 'district']='Everett' # Main Street at Carter Street\n",
    "total_station_data.at[288, 'district']='Everett' # Wasgatt Playground\n",
    "total_station_data.at[328, 'district']='Cambridge' # Mobile Temporary Station\n",
    "total_station_data.at[335, 'district']='Boston' # Warehouse Lab PBSC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill the districts found via the Google Maps API for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stations with missing district and load JSON data from above\n",
    "rows_with_nan = total_station_data[total_station_data.isnull().any(axis=1)]\n",
    "data = json.load(open(GOOGLE_MAPS_DATA_FILE))\n",
    "\n",
    "# For each record with a missing district we lookup the corresponding station id in the JSON file\n",
    "for index, row in rows_with_nan.iterrows():\n",
    "    station_id = row.station_id\n",
    "    json_station_data = data[str(station_id)][\"candidates\"][0]\n",
    "    original_station = total_station_data[total_station_data.station_id == station_id]\n",
    "    \n",
    "    # We  split the data to find the district, as the addresses found are connected with a comma. \n",
    "    # Sometimes no direct address is provided, so we need to check the length of the split.\n",
    "    # If the length is 4, an address is provided, if its 3, then no address is provided\n",
    "    formatted_address = json_station_data[\"formatted_address\"].split(\",\")\n",
    "    district = formatted_address[0].strip() if len(formatted_address) == 3 else formatted_address[1].strip()\n",
    "\n",
    "    # We will fill out the original dataframe with the JSON data\n",
    "    total_station_data.at[total_station_data.station_id == station_id, \"district\"] = district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in total_station_data\n",
    "total_station_data[total_station_data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dtypes to stations\n",
    "total_station_data = total_station_data.astype(STATION_DTYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we count the trips for each station, so we can visualize it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count amount of trips for starting and ending stations\n",
    "start_station_count = pd.DataFrame(trip_data_website.start_station_id.value_counts())\n",
    "end_station_count = pd.DataFrame(trip_data_website.end_station_id.value_counts())\n",
    "\n",
    "# Rename columns to fitting names\n",
    "start_station_count = start_station_count.rename(columns={'start_station_id':'start_station_count'})\n",
    "end_station_count = end_station_count.rename(columns={'end_station_id':'end_station_count'})\n",
    "\n",
    "# Merge station counts into station data\n",
    "total_station_data = total_station_data.merge(start_station_count, right_index=True, left_on='station_id')\n",
    "total_station_data = total_station_data.merge(end_station_count, right_index=True, left_on='station_id')\n",
    "\n",
    "# Calculate total amount of trips for each station\n",
    "total_station_data['total_station_count'] = total_station_data['start_station_count'] + total_station_data['end_station_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_station_data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data sets are the important ones for later use:\n",
    "\n",
    "- **total_station_data**: all data provided on each station\n",
    "- **trip_data_website**: all trips with weather data and station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_website.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plotting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fleet usage during a day (How many bikes were rented per hour during the day on average over the whole year 2019?)\n",
    "hours = trip_data_website[['start_time', 'precip']]\n",
    "hours['start_time'] = pd.to_timedelta(hours['start_time'].dt.hour, unit='H')\n",
    "\n",
    "flusage_day = hours.groupby([\"start_time\", \"precip\"]).size().reset_index(name='counts')\n",
    "flusage_day['counts'] = flusage_day['counts'].div(365)\n",
    "\n",
    "concatedGraphs = pd.concat(\n",
    "    [flusage_day[flusage_day['precip'] == 1].reset_index()[\"counts\"].rename(\"with Precip\"), \n",
    "     flusage_day[flusage_day['precip'] == 0].reset_index()[\"counts\"].rename(\"without Precip\"),\n",
    "    flusage_day.groupby([\"start_time\"]).sum().reset_index()[\"counts\"].rename(\"total\")],\n",
    "    axis=1)\n",
    "\n",
    "concatedGraphs.plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fleet usage during a week (How many bikes were rented per day during the week on average over the whole year 2019?)\n",
    "\n",
    "weeks = trip_data_website['start_time'].dt.day_name()\n",
    "flusage_week = weeks.groupby(weeks).size().reset_index(name='counts')\n",
    "flusage_week = flusage_week.reindex([1,5,6,4,0,2,3])\n",
    "fig = plt.figure(figsize=(17,10))\n",
    "\n",
    "bins = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "plt.title('Total number of bikes rented per weekday in the year 2019')\n",
    "plt.xlabel('Weekdays')\n",
    "plt.ylabel('Total number of rented bikes')\n",
    "plt.bar(flusage_week['start_time'], flusage_week['counts'], color='b', tick_label=bins)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fleet usage during the year 2019 (How many bikes were rented per month during the year 2019?)\n",
    "\n",
    "months = trip_data_website['start_time'].dt.month_name()\n",
    "flusage_month = months.groupby(months).size().reset_index(name='counts')\n",
    "flusage_month = flusage_month.reindex([4,3,7,0,8,6,5,1,11,10,9,2])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(17,10))\n",
    "\n",
    "plt.title('Total number of bikes rented per month in the year 2019')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total number of rented bikes')\n",
    "plt.bar(flusage_month['start_time'], flusage_month['counts'], color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predictive Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will predict the total system-level demand in the next hour. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "837837d9e5bd329943e24bf6e32a4568db8522eb54131c264e9830523dbf641c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
